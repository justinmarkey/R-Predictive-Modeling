---
title: "Random Forest Demonstration"
output: word_document
---

## Random Forest Demonstration

We will use "Adult.csv" data to perform a random forest analysis.

```{r}
dt<-read.csv("adult.csv")
dt$ABOVE50K<-as.factor(dt$ABOVE50K)
```
## Implementation of the algorithm by "randomForest" package.
```{r}
library(randomForest)
default_forest<-randomForest(data=dt,
                             ABOVE50K~.)
```

## Examine the output
We can examine the following elements from the output file:
1) Convergence of the :
```{r}
plot(default_forest)
```
*Findings:* the algrorithm converges with less than 100 trees. Therefore, if we need to fine tune the hyperparameter, the number of trees, using 100 will be enough as it can save time for running the algorithm.

2) Variable importance:
first approach
```{r}
default_forest$importance
```
second way:
```{r}
varImpPlot(default_forest)
```
3) Other features:
```{r}
default_forest
```
A detailed information about the error rate ~ number of trees is stored in err.rate object in the forest object.
```{r}
default_forest$err.rate
```

## Tune the random forest
There are two hyper-parameter of the random forest, number of trees (ntree), and how many variables should be used to build each tree (mtry).

Let's examine what are the "optimal" values for those two hyper-parameters.
We can choose a range of mtry and ntree to build different random forests, then we compare those forests to decide witho forest produces the best prediction. The corresponding parameter is the "optimal" setting for the algorithm.

We can select mtry between 2 and 7.
We can select ntree to be 100 or 150 (you can do more exploration, however, the algorithm itself can be time-consuming).

This setting gives us 6 tims 2 = 12 different models. We can create a table to summarize the performance of random forests with different parameters. This table should have three columns: 1st column indicates mtry, 2nd column indicates ntree, 3rd column indicates performance (oob forecast error).

```{r}
# Paramter Setting
para_mtry<-2:7
para_ntree<-c(100,150)
# Performance Table for saving key information, initially, all missing.
total_rows<-length(para_mtry)*length(para_ntree)
model_performance<-data.frame(
  mtry = rep(NA,total_rows),
  ntree = rep(NA,total_rows),
  oob_err = rep(NA,total_rows)
)
# iteraction indicator (indicates how many forests have been built)
iter_indicator = 0

# looping across all settings
for(i in 1:length(para_mtry)){
  for(j in 1:length(para_ntree)){
    this_iteration_mtry = para_mtry[i]
    this_iteration_ntree = para_ntree[j]
    iter_indicator = iter_indicator+1 # after each iteration, iter_indicator increases by 1
    this_forest<-randomForest(data=dt,
                 ABOVE50K~.,
                 ntree=this_iteration_ntree,
                 mtry=this_iteration_mtry)
    # find oob error
    this_iteration_error<-this_forest$err.rate[this_iteration_ntree,1]
    # update the performance table
    model_performance$mtry[iter_indicator]=this_iteration_mtry
    model_performance$ntree[iter_indicator]=this_iteration_ntree
    model_performance$oob_err[iter_indicator]=this_iteration_error
  }
}
```

The overall summary of the forests can be given by the model_performance table:
```{r}
model_performance
```

We can find that the "optimal" choice of mtry and ntree is mtry=2 and ntree=100. 

Growing forests with larger mtry results in trees to very similar to each other, which decreases the performance. Growing forests with larger ntree results in more time to complete the agorithm with no improvement in the accuracy. 



