---
title: "Performance Evaluation (General Ways)"
output: word_document
---

## Build the Logistic Regression with Admission Data
```{r}
admit.data<-read.csv("admission.csv")
admit.data$rank<-factor(admit.data$rank)
set.seed(1)
row_index<-sample(1:nrow(admit.data),300)
train<-admit.data[row_index,]
valid<-admit.data[-row_index,]
admit.logit<-glm(data=train,
                 admit~.,
                 family=binomial)
```

## 01) Confusion Matrix

Assume that prediction(probability) is greater than 50%, we define the prediction to be "Yes" (admit=1, being admitted).
```{r}
cutoff<-0.5
actual<-train$admit
pred<-predict(admit.logit, type="response")
confusion.matrix<-table(actual, pred>cutoff)
```

Now, we can calculate accuracy, sensitivity, and specificity.
Approach 1: calculate based on the confusion matrix
```{r}
# Accuracy
sum(diag(confusion.matrix))/sum(confusion.matrix)
# Sensitivity
confusion.matrix[2,2]/sum(confusion.matrix[2,])
# Specificity
confusion.matrix[1,1]/sum(confusion.matrix[1,])
```

Approach 2 (Recommended): calculate based on the definition
```{r}
# Accuracy
mean(ifelse((pred>cutoff&actual==1)|(pred<cutoff&actual==0),1,0))
# Sensitivity
sum(ifelse(pred>cutoff&actual==1,1,0))/sum(actual)
# Specificity
sum(ifelse(pred<cutoff&actual==0,1,0))/(length(actual)-sum(actual))
```

## Create a list of cutoff probabilities to determine the optimal cutoff value
```{r}
n<-1000
prob<-(1:n)/n
accuracy<-rep(NA,n)
sensitivity<-rep(NA,n)
specificity<-rep(NA,n)
for(i in 1:n){
  cutoff<-prob[i]
  accuracy[i]<-mean(ifelse((pred>cutoff&actual==1)|(pred<cutoff&actual==0),1,0))
  sensitivity[i]<-sum(ifelse(pred>cutoff&actual==1,1,0))/sum(actual)
  specificity[i]<-sum(ifelse(pred<cutoff&actual==0,1,0))/(length(actual)-sum(actual))
}

misclassification.table<-cbind.data.frame(prob, accuracy, sensitivity, specificity)
```

## 02) ROC
Based on the ROC curve definition, we can use the previous output to produce an ROC curve.
```{r}
misclassification.table$false.positive.rate<-with(misclassification.table,
                                                  1-specificity)
with(misclassification.table,
     plot(y=sensitivity,
          x=false.positive.rate,
          type = 'l'))
```

Alternatively, we can use the "ROSE" package to perform the analysis, it can plot the ROC curve and calcualte the AUC.
```{r}
library(ROSE)
roc.curve(actual, pred)
```

## 3) Repeat the previous two calcualtions using out-of-sample data
```{r}
cutoff<-0.5
actual.os<-valid$admit
pred.os<-predict(admit.logit, newdata=valid, type="response")
```

The confusion matrix can be produced by the following code:
```{r}
table(actual.os, pred.os>cutoff)
```

The accuracy, sensitivity, and specificity can be produced by the following codes:
```{r}
# Accuracy
mean(ifelse((pred.os>cutoff&actual.os==1)|(pred.os<cutoff&actual.os==0),1,0))
# Sensitivity
sum(ifelse(pred.os>cutoff&actual.os==1,1,0))/sum(actual.os)
# Specificity
sum(ifelse(pred.os<cutoff&actual.os==0,1,0))/(length(actual.os)-sum(actual.os))
```
The out-of-sample ROC curve can be easily reproduced by:
```{r}
roc.curve(actual.os, pred.os)
```



