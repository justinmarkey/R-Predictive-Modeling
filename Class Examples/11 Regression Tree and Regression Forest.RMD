---
title: "11 Regression Tree and Regression Forest"
author: "Prof. Tao Li"
output: pdf_document
---

This analysis will focus on the regression and random forest method with a numeric target variable. We want to predict the sale price of a used car.

# Step 1: Import the data
```{r}
dt<-read.csv("car sale.csv")
```

# Step 2: Apply necessary variable tranformation
Replace the *Year* variable by the *Age* of the car.
```{r}
dt$Age<-2018-dt$Year
dt<-dt[,-which(names(dt)=="Year")] # delete the original Year variable
```

# Step 3: Summarize the data
```{r}
summary(dt)
```

# Step 4: Build different models:

## 4.1 Benchmark model: Linear Regression (model_0)
I will use all remaining variables in the data as predictors in the benchmark model (model_0), which is a linear regression with only main effects.
```{r}
model_0<-lm(Selling_Price~.,data=dt)
summary(model_0)
```

## 4.2 Modified Linear Regression (model_1)
I will assume all possible two-way interactions and all main effects; and I will use stepwise selection method to get the "best" model structure. This model will be saved as (model_1).
```{r}
model_null<-lm(Selling_Price~1,data=dt)
model_full<-lm(Selling_Price~.^2,data=dt)
model_1<-step(model_full,scope=list(lower=model_null,upper=model_full),
              direction="backward",trace=FALSE)
summary(model_1)
```

## 4.3 Regression Tree (model_2)
In this regression, I will first create default tree to get an idea about the potential performance.
```{r}
library(rpart)
default_tree<-rpart(data=dt,
                    Selling_Price~.,
                    method="anova")
library(rattle)
fancyRpartPlot(default_tree)
default_tree$cptable
```
Then I will consider to increas the complexity of this default tree, and then back prune the tree to derive the fianl regression tree model.
```{r}
complex_tree<-rpart(data=dt,
                    Selling_Price~.,
                    method="anova",
                    control=rpart.control(cp=0.00001))
plotcp(complex_tree)
```
Based on this CP plot, it seems that choosing CP=0.002 may give the "optimal" tree structure.
```{r}
model_2<-prune(complex_tree,cp=0.002)
fancyRpartPlot(model_2)
```
## 4.4 Regression Forest (model_3)
I will first use the default setting to get an idea about ntree option.
```{r}
library(randomForest)
default_rf<-randomForest(data=dt,
                         Selling_Price~.)
plot(default_rf)
```

Based on the output, it seems that ntree=300 will be sufficient to reach the convergency.
Then we change mtry from 2 to 6, to find the "optimal" mtry option.

```{r}
oob.err<-double()
for (i in 1:5){
  rf<-randomForest(data=dt,Selling_Price~.,
                   ntree=300,
                   mtry=(i+1))
  oob.err[i]<-rf$mse[300]
}
oob.err
```

After trying this code block for several times, it seems that mtry=5 or mtry=6 will give the best performance. I will use mtry=5 and ntree=300 to develop the final regression forest.
```{r}
set.seed(12)
model_3<-randomForest(data=dt,
                      Selling_Price~.,
                      ntree=300,
                      mtry=5)
plot(model_3)
```
# Compare the models and determine which model fits the data best

## R-Square (is meaning, but might not be so suitable, as it gives incorrect conclusion in the comparion of model_0 and model_1)
```{r}
# Model_0:
summary(model_0)$r.squared
# Model_1:
summary(model_1)$r.squared
# Model_2:
actual<-dt$Selling_Price
pred2<-predict(model_2)
1-sum((actual-pred2)^2)/sum((actual-mean(actual))^2)
# Model_3:
pred3<-predict(model_3)
1-sum((actual-pred3)^2)/sum((actual-mean(actual))^2)
```

## Forecasting Error: RMSE
```{r}
# Model 0:
pred0<-predict(model_0)
sqrt(mean((pred0-actual)^2))
# Model 1:
pred1<-predict(model_1)
sqrt(mean((pred1-actual)^2))
# Model 2:
pred2<-predict(model_2)
sqrt(mean((pred2-actual)^2))
# Model 3:
sqrt(mean((pred3-actual)^2))
```

**Conclusions**: *Model_1* has the best performs, in terms of both r-squared and rmse. It generates the most accurate prediction for this data.
