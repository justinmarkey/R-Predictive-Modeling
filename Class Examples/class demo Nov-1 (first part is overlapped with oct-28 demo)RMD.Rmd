---
title: "Performance Evaluation (General Ways)"
output: word_document
---

## Build the Logistic Regression with Admission Data
```{r}
admit.data<-read.csv("admission.csv")
admit.data$rank<-factor(admit.data$rank)
set.seed(1)
row_index<-sample(1:nrow(admit.data),300)
train<-admit.data[row_index,]
valid<-admit.data[-row_index,]
admit.logit<-glm(data=train,
                 admit~.,
                 family=binomial)
```

## 01) Confusion Matrix

Assume that prediction(probability) is greater than 50%, we define the prediction to be "Yes" (admit=1, being admitted).
```{r}
cutoff<-0.5
actual<-train$admit
pred<-predict(admit.logit, type="response")
confusion.matrix<-table(actual, pred>cutoff)
```

Now, we can calculate accuracy, sensitivity, and specificity.
Approach 1: calculate based on the confusion matrix
```{r}
# Accuracy
sum(diag(confusion.matrix))/sum(confusion.matrix)
# Sensitivity
confusion.matrix[2,2]/sum(confusion.matrix[2,])
# Specificity
confusion.matrix[1,1]/sum(confusion.matrix[1,])
```

Approach 2 (Recommended): calculate based on the definition
```{r}
# Accuracy
mean(ifelse((pred>cutoff&actual==1)|(pred<cutoff&actual==0),1,0))
# Sensitivity
sum(ifelse(pred>cutoff&actual==1,1,0))/sum(actual)
# Specificity
sum(ifelse(pred<cutoff&actual==0,1,0))/(length(actual)-sum(actual))
```

## Create a list of cutoff probabilities to determine the optimal cutoff value
```{r}
n<-1000
prob<-(1:n)/n
accuracy<-rep(NA,n)
sensitivity<-rep(NA,n)
specificity<-rep(NA,n)
for(i in 1:n){
  cutoff<-prob[i]
  accuracy[i]<-mean(ifelse((pred>cutoff&actual==1)|(pred<cutoff&actual==0),1,0))
  sensitivity[i]<-sum(ifelse(pred>cutoff&actual==1,1,0))/sum(actual)
  specificity[i]<-sum(ifelse(pred<cutoff&actual==0,1,0))/(length(actual)-sum(actual))
}

misclassification.table<-cbind.data.frame(prob, accuracy, sensitivity, specificity)
```

## 02) ROC
Based on the ROC curve definition, we can use the previous output to produce an ROC curve.
```{r}
misclassification.table$false.positive.rate<-with(misclassification.table,
                                                  1-specificity)
with(misclassification.table,
     plot(y=sensitivity,
          x=false.positive.rate,
          type = 'l'))
```

Alternatively, we can use the "ROSE" package to perform the analysis, it can plot the ROC curve and calcualte the AUC.
```{r}
library(ROSE)
roc.curve(actual, pred)
```

## 3) Repeat the previous two calcualtions using out-of-sample data
```{r}
cutoff<-0.5
actual.os<-valid$admit
pred.os<-predict(admit.logit, newdata=valid, type="response")
```

The confusion matrix can be produced by the following code:
```{r}
table(actual.os, pred.os>cutoff)
```

The accuracy, sensitivity, and specificity can be produced by the following codes:
```{r}
# Accuracy
mean(ifelse((pred.os>cutoff&actual.os==1)|(pred.os<cutoff&actual.os==0),1,0))
# Sensitivity
sum(ifelse(pred.os>cutoff&actual.os==1,1,0))/sum(actual.os)
# Specificity
sum(ifelse(pred.os<cutoff&actual.os==0,1,0))/(length(actual.os)-sum(actual.os))
```
The out-of-sample ROC curve can be easily reproduced by:
```{r}
roc.curve(actual.os, pred.os)
```


# 04) Lift Charts and Gain Tables.
First, Let's produce a lift chart to evaluate the in-sample performance of the model.

We need to first find the key inputs
```{r}
actual_y<-train$admit
pred_y<-predict(admit.logit, type="response")
```

Then we can translate prediction into model score decile.
```{r}
library(dplyr)
decile_pred<-ntile(1-pred_y, n=10)
```

We can put all the information into one dataset
```{r}
summary_data<-data.frame(
  actual_y=actual_y,
  pred_y=pred_y,
  model_decile=decile_pred
)
```

Then summarize by model decile
```{r}
lift_table<-summary_data %>% 
  group_by(model_decile) %>%
  summarise(
    n=n(),
    resp_n=sum(actual_ytal_y
    resp_rate=mean(actual_y))
```

Then we can add cumulative summary, we can do that with looping across each row
```{r}
cumulative_n<-rep(NA,nrow(lift_table))
cumulative_resp_n<-rep(NA,nrow(lift_table))
for(i in 1:nrow(lift_table)){
  cumulative_resp_n[i]=sum(lift_table$resp_n[1:i])
  cumulative_n[i]=sum(lift_table$n[1:i])
}
cumulative_resp<-cumulative_resp_n/cumulative_n
cumulative_lift<-cumulative_resp/mean(actual_y)

lift_table$cumulative_resp<-cumulative_resp
lift_table$cumulative_lift<-cumulative_lift
```

We can show the complete lift chart:
```{r}
lift_table
barplot(lift_table$cumulative_lift)
```

Alternatively, we can use an existing package call "gains".
```{r}
library(gains)
gains_output<-gains(actual=actual_y,
      predicted=pred_y,
      groups=10)
barplot(gains_output$cume.lift)
barplot(gains_output$lift)
```





